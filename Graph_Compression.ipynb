{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports, Initialization, and Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\colef\\AppData\\Local\\Temp\\ipykernel_22792\\678055449.py:1: DeprecationWarning: \n",
      "Pyarrow will become a required dependency of pandas in the next major release of pandas (pandas 3.0),\n",
      "(to allow more performant data types, such as the Arrow string type, and better interoperability with other libraries)\n",
      "but was not found to be installed on your system.\n",
      "If this would cause problems for you,\n",
      "please provide us feedback at https://github.com/pandas-dev/pandas/issues/54466\n",
      "        \n",
      "  import pandas as pd\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import numpy as np\n",
    "import math\n",
    "import scipy.linalg\n",
    "import operator as op\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Hyperparameters\n",
    "\n",
    "#K: Maximum hop distance (K = 2)\n",
    "K = 2\n",
    "\n",
    "#Gamma: Hop discount factor (γ = 0.01)\n",
    "gamma = 0.01\n",
    "\n",
    "#Eta: Node degree threshold (η = 15)\n",
    "eta = 15\n",
    "\n",
    "#Phi: Compression ratio (φ = 0.2)\n",
    "phi = 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import PPI Data and construct protein set\n",
    "data = pd.read_csv(r\"C:\\Users\\colef\\OneDrive - University of Miami\\Documents\\College\\Research\\Wuchty Lab\\Data\\HI-union.tsv\",sep='\\t')\n",
    "data.drop(data[(data['Protein1'] == data['Protein2'])].index,inplace=True)\n",
    "data.reset_index(drop=True,inplace=True)\n",
    "protein_set = set([*data.Protein1,*data.Protein2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_neighbors(protein,data):\n",
    "    rows = data.loc[(data['Protein1'] == protein) | (data['Protein2'] == protein)]\n",
    "    neighbors = [*rows.Protein1,*rows.Protein2]\n",
    "    neighbors = set(neighbors)\n",
    "    neighbors.remove(protein)\n",
    "    return neighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_degree(protein,neighbor_dict):\n",
    "    return len(neighbor_dict[protein])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_k_neighbors(protein,neighbor_dict,k):\n",
    "    neighbor_set = neighbor_dict[protein].copy()\n",
    "    while k > 1:\n",
    "        for protein in neighbor_set:\n",
    "            temp_neighbors = neighbor_dict[protein]\n",
    "            neighbor_set = neighbor_set | temp_neighbors\n",
    "        k -= 1\n",
    "    return neighbor_set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initialize neighbor dictionary\n",
    "neighbor_dict = {}\n",
    "for protein in protein_set:\n",
    "    neighbor_dict[protein] = get_neighbors(protein,data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initialize degree dictionary and max_degree variable\n",
    "degree_dict = {}\n",
    "for protein in protein_set:\n",
    "    degree_dict[protein] = get_degree(protein,neighbor_dict)\n",
    "max_degree = max(degree_dict.values())\n",
    "\n",
    "# Validate with degree-edge law\n",
    "if sum(degree_dict.values()) != 2*len(data):\n",
    "    print(\"ERROR\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_s_features(protein):\n",
    "    features = torch.zeros(1,round(math.log2(max_degree))+1)\n",
    "    k = 1\n",
    "    while k <= K:\n",
    "        #Get neighbors within that hop distance\n",
    "        neighbors =  get_k_neighbors(protein,neighbor_dict,k)\n",
    "        neighbors.add(protein)\n",
    "        temporary_features = np.zeros([1,round(math.log2(max_degree))+1])\n",
    "        for neighbor in neighbors:\n",
    "            idx = round(math.log2(degree_dict[neighbor]))\n",
    "            temporary_features[0][idx] += 1\n",
    "        temporary_features = pow(gamma,k-1)*temporary_features\n",
    "        features += temporary_features\n",
    "        k += 1\n",
    "    return features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get features for all proteins\n",
    "s_features = torch.zeros(len(protein_set),round(math.log2(max_degree))+1,dtype=torch.float64)\n",
    "for row,protein in enumerate(protein_set):\n",
    "    temp_features = get_s_features(protein)\n",
    "    s_features[row] = temp_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Embedding Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GCN_layer(adjacency,features,weights):\n",
    "    AF = torch.matmul(adjacency,features)\n",
    "    AFW = torch.matmul(AF,weights)\n",
    "    output = torch.tanh(AFW)\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Weight matrices are randomly initialized based on Glorot and Bengio approach\n",
    "def initialize_weights(previous_size,current_size):\n",
    "    dist = [-math.sqrt(6)/math.sqrt(current_size[1]+previous_size[1]),math.sqrt(6)/math.sqrt(current_size[1]+previous_size[1])]\n",
    "    weights = np.random.rand(previous_size[1],current_size[1])\n",
    "    weights = (dist[1] - dist[0]) * weights + dist[0]\n",
    "    return torch.tensor(weights).double()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embedding Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initialize Adjacency matrix (Ajoin + I), and Degree matrix (D)\n",
    "protein_list = list(protein_set)\n",
    "Ajoin = torch.zeros(len(protein_list),len(protein_list),dtype=torch.float64)\n",
    "D = torch.zeros(len(protein_list),len(protein_list),dtype=torch.float64)\n",
    "for i,protein in enumerate(protein_list):\n",
    "    neighbors = neighbor_dict[protein]\n",
    "    for neighbor in neighbors:\n",
    "        Ajoin[protein_list.index(protein)][protein_list.index(neighbor)] = 1\n",
    "    Ajoin[i][i] = 1\n",
    "    D[i][i] = degree_dict[protein]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Compute ^Ajoin using Ajoin and D\n",
    "D2 = torch.from_numpy(scipy.linalg.fractional_matrix_power(D,-1/2)).double()\n",
    "temp_result = torch.matmul(D2,Ajoin)\n",
    "Ajoin = torch.matmul(temp_result,D2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#GCN Layer 1\n",
    "features_size = s_features.size()\n",
    "hidden_size = [round(math.log2(max_degree))+1,round(math.log2(max_degree))+1]\n",
    "hidden_weights = initialize_weights(features_size,hidden_size)\n",
    "H1 = GCN_layer(Ajoin,s_features,hidden_weights)\n",
    "\n",
    "#GCN Layer 2\n",
    "H1_size = H1.size()\n",
    "embedding_size = [hidden_size[0]*2,hidden_size[0]*2]\n",
    "embedding_weights = initialize_weights(H1_size,embedding_size)\n",
    "output = GCN_layer(Ajoin,H1,embedding_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initialize Feature dictionary\n",
    "feature_dict = {}\n",
    "for i,features in enumerate(output):\n",
    "    protein = protein_list[i]\n",
    "    feature_dict[protein] = features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Guiding List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reduce size of protein list based on degree threshold η\n",
    "reduced_protein_list = []\n",
    "reduced_feature_dict = {}\n",
    "for i,protein in enumerate(protein_list):\n",
    "    if degree_dict[protein] > eta:\n",
    "        reduced_protein_list.append(protein)\n",
    "        reduced_feature_dict[protein] = feature_dict[protein]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initialize list of norms\n",
    "norm_list = torch.zeros(len(reduced_protein_list))\n",
    "for i,features in enumerate(reduced_feature_dict.values()):\n",
    "    norm_list[i] = torch.linalg.vector_norm(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sort lists\n",
    "(sorted_norm_list,arglist) = norm_list.sort(descending=True)\n",
    "sorted_protein_list = [None]*(max(arglist)+1)\n",
    "for i,row in enumerate(arglist):\n",
    "    sorted_protein_list[row] = reduced_protein_list[i]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Graph Compression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_from_dicts(subnode,compressed_data,compressed_neighbor_dict,compressed_degree_dict,guided_list):\n",
    "    compressed_data.drop(compressed_data[(compressed_data['Protein1'] == subnode) | (compressed_data['Protein2'] == subnode)].index,inplace=True)\n",
    "    compressed_data.reset_index(drop=True,inplace=True)\n",
    "    del compressed_neighbor_dict[subnode]\n",
    "    del compressed_degree_dict[subnode]\n",
    "    for node in compressed_neighbor_dict.keys():\n",
    "        if subnode in compressed_neighbor_dict[node]:\n",
    "            compressed_neighbor_dict[node].remove(subnode)\n",
    "            compressed_degree_dict[node] = compressed_degree_dict[node] - 1\n",
    "    if subnode in guided_list:\n",
    "        guided_list.remove(subnode)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Graph Compression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initializations\n",
    "compress_rate = 0\n",
    "compressed_data = data.copy()\n",
    "compressed_neighbor_dict = neighbor_dict.copy()\n",
    "compressed_degree_dict = degree_dict.copy()\n",
    "guided_list = copy.copy(sorted_protein_list)\n",
    "\n",
    "#Compress loop\n",
    "while compress_rate < phi:\n",
    "    \n",
    "    #Iterate through guided list\n",
    "    for protein in guided_list:\n",
    "        \n",
    "        #Get minimum degree of neighbors\n",
    "        neighbor_list = compressed_neighbor_dict[protein]\n",
    "        neighbor_degree_dict = {}\n",
    "        for neighbor in neighbor_list:\n",
    "            neighbor_degree_dict[neighbor] = compressed_degree_dict[neighbor]\n",
    "        min_degree = min(neighbor_degree_dict.values())\n",
    "        \n",
    "        #Make list of nodes for compression based on min_degree\n",
    "        compress_list = [protein]\n",
    "        for neighbor in neighbor_list:\n",
    "            if neighbor_degree_dict[neighbor] == min_degree:\n",
    "                compress_list.append(neighbor)\n",
    "        \n",
    "        #If there are nodes to compress, do so\n",
    "        if len(compress_list) > 1:\n",
    "            \n",
    "            supernode_neighbors = set()\n",
    "            supernode = '+'.join(compress_list)\n",
    "            compress_list = list(set(compress_list))\n",
    "            \n",
    "            #Remove subnodes and subnode connections from data and guiding list\n",
    "            for subnode in compress_list:\n",
    "                \n",
    "                #Get subnode neighbors\n",
    "                node_neighbors = compressed_neighbor_dict[subnode]\n",
    "                supernode_neighbors = supernode_neighbors | node_neighbors\n",
    "            \n",
    "                #Remove subnodes from graph, guided_list and dicts\n",
    "                remove_from_dicts(subnode,compressed_data,compressed_neighbor_dict,compressed_degree_dict,guided_list)\n",
    "                \n",
    "            #Remove neighbors that will be a part of the supernode\n",
    "            supernode_neighbors = supernode_neighbors - set(compress_list)\n",
    "            \n",
    "            #Add supernode connections to compress_list, update dicts, and append to guided list\n",
    "            for neighbor in supernode_neighbors:\n",
    "                compressed_data.loc[len(compressed_data.index)] = [supernode,neighbor]\n",
    "                compressed_neighbor_dict[neighbor] = compressed_neighbor_dict[neighbor] | set([supernode])\n",
    "                compressed_degree_dict[neighbor] = compressed_degree_dict[neighbor] + 1\n",
    "            guided_list.append(supernode)\n",
    "            \n",
    "            #Update dicts\n",
    "            compressed_neighbor_dict[supernode] = supernode_neighbors\n",
    "            compressed_degree_dict[supernode] = get_degree(supernode,compressed_neighbor_dict)\n",
    "        \n",
    "        # Validate with degree-edge law\n",
    "        if sum(compressed_degree_dict.values()) != 2*len(compressed_data):\n",
    "            print('ERROR')\n",
    "            \n",
    "        #Adjust compress_rate and check value against phi\n",
    "        compress_rate = 1 - len(compressed_data.index)/len(data.index)\n",
    "        if compress_rate >= phi:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get set of supernodes and their subnodes\n",
    "compressed_protein_set = set([*compressed_data.Protein1,*compressed_data.Protein2])\n",
    "supernode_dict = {}\n",
    "for protein in compressed_protein_set:\n",
    "    if '+' in protein:\n",
    "        supernode_dict[protein] = set(protein.split('+'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save compressed graph\n",
    "compressed_data.to_csv('HI-union_compressed.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
